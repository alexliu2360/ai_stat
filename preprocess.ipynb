{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/alexliu-ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Embedding, np\n",
    "from keras.utils import to_categorical\n",
    "from keras_applications.densenet import layers\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/trainingset/sentiment_analysis_trainingset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location_traffic_convenience',\n",
       " 'location_distance_from_business_district',\n",
       " 'location_easy_to_find',\n",
       " 'service_wait_time',\n",
       " 'service_waiters_attitude',\n",
       " 'service_parking_convenience',\n",
       " 'service_serving_speed',\n",
       " 'price_level',\n",
       " 'price_cost_effective',\n",
       " 'price_discount',\n",
       " 'environment_decoration',\n",
       " 'environment_noise',\n",
       " 'environment_space',\n",
       " 'environment_cleaness',\n",
       " 'dish_portion',\n",
       " 'dish_taste',\n",
       " 'dish_look',\n",
       " 'dish_recommendation',\n",
       " 'others_overall_experience',\n",
       " 'others_willing_to_consume_again']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(jieba.cut(data['content'][0]))[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准备数据集\n",
    "train_data = pd.read_csv('../data/trainingset/sentiment_analysis_trainingset.csv')\n",
    "val_data = pd.read_csv('../data/validationset/sentiment_analysis_validationset.csv')\n",
    "testa_data = pd.read_csv('../data/testa/sentiment_analysis_testa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_stoplist():\n",
    "    # 准备停用词\n",
    "    with open('./stopwords.txt') as f:\n",
    "        stoplist = f.read().split()\n",
    "    stoplist.append(str(\"\\n\"))\n",
    "    stoplist.append(\" \")\n",
    "    return stoplist\n",
    "\n",
    "# 去掉停用词\n",
    "def remove_stopword(data, stoplist):\n",
    "    texts = []\n",
    "    for index, row in data.iterrows():\n",
    "        line = [word for word in list(row['words']) if word not in stoplist]\n",
    "        texts.append(line)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分小额数据做训练以及验证， 这部分可以不再使用\n",
    "train_counts = 500\n",
    "val_counts = 100\n",
    "test_counts = 200\n",
    "data_ls = data.loc[:train_counts + val_counts + test_counts-1, :]\n",
    "data_ls['words'] = data_ls['content'].apply(lambda x: list(jieba.cut(x)))\n",
    "y_cols = list(data_ls.columns)[2:]\n",
    "print(data_ls.shape)\n",
    "train = data_ls.loc[:train_counts-1, :]\n",
    "val = data_ls.loc[train_counts:train_counts+val_counts-1, :]\n",
    "test = data_ls.loc[train_counts+val_counts:train_counts+val_counts+test_counts-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jieba分词\n",
    "train_data['words'] = train_data['content'].apply(lambda x: list(jieba.cut(x)))\n",
    "val_data['words'] = testa_data['content'].apply(lambda x: list(jieba.cut(x)))\n",
    "testa_data['words'] = testa_data['content'].apply(lambda x: list(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉content列\n",
    "%time\n",
    "train_data = train_data.drop(columns=['content'], axis=1)\n",
    "val_data = val_data.drop(columns=['content'], axis=1)\n",
    "testa = testa_data.drop(columns=['content'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除停词\n",
    "stoplist = make_stoplist()\n",
    "train_data['texts'] = pd.Series(remove_stopword(train_data, stoplist))\n",
    "val_data['texts'] = pd.Series(remove_stopword(val_data, stoplist))\n",
    "testa_data['texts'] = pd.Series(remove_stopword(testa_data, stoplist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉words列\n",
    "%time\n",
    "train_data = train_data.drop(columns=['words'], axis=1)\n",
    "val_data = val_data.drop(columns=['words'], axis=1)\n",
    "testa = testa_data.drop(columns=['words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预处理的数据放入到相应的csv中\n",
    "train_data.to_csv('../data/trainingset/dealed_trainingset.csv')\n",
    "val_data.to_csv('../data/validationset/dealed_validationset.csv')\n",
    "testa_data.to_csv('../data/testa/dealed_testa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
