{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/alexliu-ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Embedding, np\n",
    "from keras.utils import to_categorical\n",
    "from keras_applications.densenet import layers\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/trainingset/dealed_trainingset.csv')\n",
    "val_data = pd.read_csv('../data/validationset/dealed_validationset.csv')\n",
    "testa_data = pd.read_csv('../data/testa/dealed_testa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求每一个样本的最大长度\n",
    "def get_maxlen(texts):\n",
    "    maxlen = 0\n",
    "    for line in texts:\n",
    "        if maxlen < len(line):\n",
    "            maxlen = len(line)\n",
    "    return maxlen\n",
    "\n",
    "# 利用keras的Tokenizer进行onehot，并调整未等长数组\n",
    "def preprocess_data(texts):\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    data_w = tokenizer.texts_to_sequences(texts)\n",
    "    data_T = sequence.pad_sequences(data_w, maxlen=get_maxlen(texts))\n",
    "    print(data_T.shape)\n",
    "    return data_T\n",
    "\n",
    "def get_texts_maxlen(args):\n",
    "    maxlen = args[0]\n",
    "    for i in range(0, len(args)):\n",
    "        if args[i] > maxlen:\n",
    "            maxlen = args[i]\n",
    "    return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105000, 5069)\n",
      "(15000, 4887)\n",
      "(15000, 4887)\n"
     ]
    }
   ],
   "source": [
    "# 数据划分，重新划分为训练集，测试集和验证集\n",
    "onehot_train_texts = preprocess_data(train_data['texts'])\n",
    "onehot_val_texts = preprocess_data(val_data['texts'])\n",
    "onehot_test_texts = preprocess_data(testa_data['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5069"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [onehot_train_texts.shape[1], onehot_test_texts.shape[1], onehot_test_texts.shape[1]]\n",
    "maxlen = get_texts_maxlen(lens)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cols = ['location_traffic_convenience',\n",
    " 'location_distance_from_business_district',\n",
    " 'location_easy_to_find',\n",
    " 'service_wait_time',\n",
    " 'service_waiters_attitude',\n",
    " 'service_parking_convenience',\n",
    " 'service_serving_speed',\n",
    " 'price_level',\n",
    " 'price_cost_effective',\n",
    " 'price_discount',\n",
    " 'environment_decoration',\n",
    " 'environment_noise',\n",
    " 'environment_space',\n",
    " 'environment_cleaness',\n",
    " 'dish_portion',\n",
    " 'dish_taste',\n",
    " 'dish_look',\n",
    " 'dish_recommendation',\n",
    " 'others_overall_experience',\n",
    " 'others_willing_to_consume_again']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    embedding_dim = 128\n",
    "    max_words = 50000\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(5))\n",
    "    model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metrics(Callback):    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_predict = np.argmax(np.asarray(self.model.predict(self.validation_data[0])), axis=1)\n",
    "#         val_targ = self.validation_data[1]\n",
    "        val_targ = np.argmax(self.validation_data[1], axis=1)\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='macro')\n",
    "#         _val_recall = recall_score(val_targ, val_predict)\n",
    "#         _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "#         self.val_recalls.append(_val_recall)\n",
    "#         self.val_precisions.append(_val_precision)\n",
    "#         print('— val_f1: %f — val_precision: %f — val_recall %f' %(_val_f1, _val_precision, _val_recall))\n",
    "        print(' — val_f1:' ,_val_f1)\n",
    "        return \n",
    "\n",
    "    \n",
    "def train_CV_CNN(train_x, test_x, val_x, y_cols=y_cols, debug=True, folds=2):\n",
    "    model = build_model()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    F1_scores = 0\n",
    "    F1_score = 0\n",
    "    metrics = Metrics()\n",
    "    if debug:\n",
    "        y_cols = ['location_traffic_convenience']\n",
    "    for index, col in enumerate(y_cols):\n",
    "        train_y = train_data[col] + 2\n",
    "        val_y = val_data[col] + 2\n",
    "        y_val_pred = 0\n",
    "        y_test_pred = 0\n",
    "        result = {}\n",
    "        for i in range(1):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=100 * i)\n",
    "            y_train_onehot = to_categorical(y_train)\n",
    "            y_test_onehot = to_categorical(y_test)\n",
    "            history = model.fit(X_train, y_train_onehot, epochs=5, batch_size=128, \n",
    "                                validation_data=(X_test, y_test_onehot),callbacks=[metrics])\n",
    "\n",
    "            # 预测验证集和测试集y_test_pred\n",
    "            y_val_pred = model.predict(val_x)\n",
    "            y_test_pred += model.predict(test_x)\n",
    "\n",
    "            y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "            F1_score = f1_score(y_val_pred, val_y, average='macro')\n",
    "            F1_scores += F1_score\n",
    "\n",
    "            print(col, 'f1_score:', F1_score, 'ACC_score:', accuracy_score(y_val_pred, val_y))\n",
    "        y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "        result[col] = y_test_pred - 2\n",
    "    print('all F1_score:', F1_scores / len(y_cols))\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv('../data/result/result.csv')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CV_CNN(train_x=onehot_train_texts, val_x=onehot_val_texts, test_x=onehot_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
